Проект завёрнут в docker, запускается через `docker-compose up`.  

API сервиса:  
`http://localhost:8000/api/key/new_keys_counter/` – Информация о количестве оставшихся, не выданных ключах.  
`http://localhost:8000/api/key/issue/` – Выдача уникального ключа.  
`http://localhost:8000/api/key/expire/<ключ>/` – Погашение ключа.  
`http://localhost:8000/api/key/check/<ключ>/` – Проверка ключа.  

Пояснение:  
Ключи генерируются на этапе миграции и хранятся в БД Postgres (миграция 0002). Это позволяет переложить проблемы параллельного доступа на БД. Чтобы быстро выдавать количество оставшихся ключей, мы кэшируем это значение в отдельной таблице (миграция 0004). И при каждом обновлении статуса ключа с "нового" на "выданный" мы уменьшаем это значение через хранимую процедуру (миграция 0005). Чтобы не возникло каких-либо расхождений между закэшированным количеством и фактическим, после генерации ключей мы запрещаем их добавление и удаление (миграция 0003).  

Плюсы и минусы такого решения:  
Минусы:  
* долгое время на разворачивание: уникальные ключи добавляются в базу около 3 минут на моём домашнем ноутбуке в докер-контейнере. Считаю, что т.к. это разовая операция и остальные операции сервис выполняет быстро, то долгим разворачиванием можно пренебречь.
* логика частично вынесена в БД в виде триггеров и хранимых процедур. Это может затруднить поддержку и тестирование.  

  
Плюсы:
* т.к. ключи заранее сгенерированы нет проблем с отслеживанием их уникальности.
* проблема race condition решается на уровне БД: база следит за тем, чтобы один и тот же ключ не был выдан два раза и чтобы не было возможности погасить один ключ дважды.
* сервис масштабируемый, можем запускать несколько веб-воркеров на python т.к. БД у нас единственный источник правды, который хранит ключи и отвечает за целостность данных.
* персистентность данных, т.к. все ключи хранятся в БД на жёстком диске.